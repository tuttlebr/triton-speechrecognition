{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gwt7z7qdmTbW"
   },
   "outputs": [],
   "source": [
    "# Copyright 2019 NVIDIA Corporation. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i4NKCp2VmTbn"
   },
   "source": [
    "<img src=\"http://developer.download.nvidia.com/compute/machine-learning/frameworks/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n",
    "\n",
    "# Kaldi TRTIS Inference Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fW0OKDzvmTbt"
   },
   "source": [
    "## Overview\n",
    "\n",
    "\n",
    "This repository provides a wrapper around the online GPU-accelerated ASR pipeline from the paper [GPU-Accelerated Viterbi Exact Lattice Decoder for Batched Online and Offline Speech Recognition](https://arxiv.org/abs/1910.10032). That work includes a high-performance implementation of a GPU HMM Decoder, a low-latency Neural Net driver, fast Feature Extraction for preprocessing, and new ASR pipelines tailored for GPUs. These different modules have been integrated into the Kaldi ASR framework.\n",
    "\n",
    "This repository contains a TensorRT Inference Server custom backend for the Kaldi ASR framework. This custom backend calls the high-performance online GPU pipeline from the Kaldi ASR framework. This TensorRT Inference Server integration provides ease-of-use to Kaldi ASR inference: gRPC streaming server, dynamic sequence batching, and multi-instances support. A client connects to the gRPC server, streams audio by sending chunks to the server, and gets back the inferred text as an answer. More information about the TensorRT Inference Server can be found [here](https://docs.nvidia.com/deeplearning/sdk/tensorrt-inference-server-guide/docs/).  \n",
    "\n",
    "\n",
    "\n",
    "### Learning objectives\n",
    "\n",
    "This notebook demonstrates the steps for carrying out inferencing with the Kaldi TRTIS backend server using a Python gRPC client in an offline context, that is, we will stream pre-recorded .wav files to the inference server and receive the results back.\n",
    "\n",
    "## Content\n",
    "1. [Pre-requisite](#1)\n",
    "1. [Setup](#2)\n",
    "1. [Audio helper classes](#3)\n",
    "1. [Inference](#4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aDFrE4eqmTbv"
   },
   "source": [
    "<a id=\"1\"></a>\n",
    "## 1. Pre-requisite\n",
    "\n",
    "### 1.1 Docker containers\n",
    "Follow the steps in [README](README.md) to build Kaldi server and client containers.\n",
    "\n",
    "### 1.2 Hadrware\n",
    "This notebook can be executed on any CUDA-enabled NVIDIA GPU, although for efficient mixed precision inference, a [Tensor Core NVIDIA GPU](https://www.nvidia.com/en-us/data-center/tensorcore/) is desired (Volta, Turing or newer architectures). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi \\\n",
    "    --query-gpu=index,name,driver_version,memory.total,uuid,mig.mode.current \\\n",
    "    --format=csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HqSUGePjmTb9"
   },
   "source": [
    "### 1.3 Data download and preprocessing\n",
    "\n",
    "The  script `scripts/docker/launch_download.sh` will download the LibriSpeech test dataset along with Kaldi ASR models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S2PR7weWmTcK"
   },
   "outputs": [],
   "source": [
    "!ls /Kaldi/data\n",
    "!ls /Kaldi/data/data/LibriSpeech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EQAIszkxmTcT"
   },
   "source": [
    "Within the docker container, the final data  and model directory should look like:\n",
    "\n",
    "```\n",
    "data  datasets\tmodels\n",
    "BOOKS.TXT     LICENSE.TXT  SPEAKERS.TXT  test-other\n",
    "CHAPTERS.TXT  README.TXT   test-clean\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a>\n",
    "## 2 Setup \n",
    "### 2.1 Import libraries and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import queue\n",
    "import subprocess\n",
    "from functools import partial\n",
    "from io import StringIO\n",
    "\n",
    "import grpc\n",
    "import IPython.display as ipd\n",
    "# import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import soundfile\n",
    "import tritonclient.grpc as grpcclient\n",
    "from tritonclient.utils import InferenceServerException"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Set Client Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path for input file. First line should contain number of lines to search in\n",
    "FILE = None\n",
    "\n",
    "# Inference server URL from LoadBalancer.\n",
    "URL = \"172.42.42.1:8001\"\n",
    "\n",
    "# Name of model\n",
    "MODEL_NAME = \"kaldi_online\"\n",
    "\n",
    "# Version of model\n",
    "MODEL_VERSION = \"1\"\n",
    "\n",
    "# Inference Batch Size\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "# Inference / Server verbosity\n",
    "VERBOSE = False\n",
    "\n",
    "# Unique ID for a request\n",
    "CORRELATION_ID = \"1101\"\n",
    "\n",
    "# Convert sample type to float32. Integers will be scaled to [-1, 1] in float32\n",
    "WAV_SCALE_FACTOR = 2**15 - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Checking server status\n",
    "\n",
    "Triton has multiple methods of collecting health status and metadata from both the server and model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create gRPC stub for communicating with the server\n",
    "channel = grpc.insecure_channel(URL)\n",
    "grpc_stub = grpcclient.service_pb2_grpc.GRPCInferenceServiceStub(channel)\n",
    "\n",
    "# Check Server Status\n",
    "request = grpcclient.service_pb2.ServerLiveRequest()\n",
    "response = grpc_stub.ServerLive(request)\n",
    "print(\"server {}\".format(response))\n",
    "\n",
    "# Check Server Metadata\n",
    "response = grpc_stub.ServerLive(request)\n",
    "request = grpcclient.service_pb2.ServerMetadataRequest()\n",
    "response = grpc_stub.ServerMetadata(request)\n",
    "print(\"server metadata:\\n{}\".format(response))\n",
    "\n",
    "# Check Model Status\n",
    "request = grpcclient.service_pb2.ModelReadyRequest(\n",
    "    name=MODEL_NAME, version=MODEL_VERSION\n",
    ")\n",
    "response = grpc_stub.ModelReady(request)\n",
    "print(\"model {}\".format(response))\n",
    "\n",
    "\n",
    "# Check Model Metadata\n",
    "request = grpcclient.service_pb2.ModelMetadataRequest(\n",
    "    name=MODEL_NAME, version=MODEL_VERSION\n",
    ")\n",
    "response = grpc_stub.ModelMetadata(request)\n",
    "print(\"model metadata:\\n{}\".format(response))\n",
    "\n",
    "\n",
    "# Check Model Configuration - output may be verbose\n",
    "request = grpcclient.service_pb2.ModelConfigRequest(\n",
    "    name=MODEL_NAME, version=MODEL_VERSION\n",
    ")\n",
    "response = grpc_stub.ModelConfig(request)\n",
    "print(\"model config:\\n{}\".format(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RL8d9IwzmTcV"
   },
   "source": [
    "<a id=\"3\"></a>\n",
    "## 3. Audio helper classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o6wayGf1mTcX"
   },
   "source": [
    "Next, we define some helper classes for pre-processing audio from files. The below AudioSegment class reads audio data from .wav files and converts the sampling rate to that required by the Kaldi ASR model, which is 16000Hz by default.\n",
    "\n",
    "Note:  For historical reasons, Kaldi expects waveforms in the range (2^15-1)x[-1, 1], not the usual default DSP range [-1, 1]. Therefore, we scale the audio signal by a factor of (2^15-1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data\n",
    "\n",
    "We load and play a wave file from the LibriSpeech data set. The LibriSpeech data set is organized into directories and subdirectories containing speech segments and transcripts for different speakers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define a helper function which generate pairs of filepath and transcript from a LibriSpeech data directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR = \"1089\"\n",
    "SUBDIR = \"134686\"\n",
    "FILE_ID = \"0000\"\n",
    "FILE_NAME = \"/Kaldi/data/data/LibriSpeech/test-clean/%s/%s/%s-%s-%s.wav\" % (\n",
    "    DIR,\n",
    "    SUBDIR,\n",
    "    DIR,\n",
    "    SUBDIR,\n",
    "    FILE_ID,\n",
    ")\n",
    "TRANSRIPTION_FILE = \"/Kaldi/data/data/LibriSpeech/test-clean/%s/%s/%s-%s.trans.txt\" % (\n",
    "    DIR,\n",
    "    SUBDIR,\n",
    "    DIR,\n",
    "    SUBDIR,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioSegment(object):\n",
    "    \"\"\"Monaural audio segment abstraction.\n",
    "    :param samples: Audio samples [num_samples x num_channels].\n",
    "    :type samples: ndarray.float32\n",
    "    :param sample_rate: Audio sample rate.\n",
    "    :type sample_rate: int\n",
    "    :raises TypeError: If the sample data type is not float or int.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, samples, sample_rate, target_sr=16000, trim=False, trim_db=60):\n",
    "        \"\"\"Create audio segment from samples.\n",
    "        Samples are convert float32 internally, with int scaled to [-1, 1].\n",
    "        \"\"\"\n",
    "        samples = self._convert_samples_to_float32(samples)\n",
    "        if target_sr is not None and target_sr != sample_rate:\n",
    "            samples = librosa.core.resample(samples, sample_rate, target_sr)\n",
    "            sample_rate = target_sr\n",
    "        if trim:\n",
    "            samples, _ = librosa.effects.trim(samples, trim_db)\n",
    "        self._samples = samples\n",
    "        self._sample_rate = sample_rate\n",
    "        if self._samples.ndim >= 2:\n",
    "            self._samples = np.mean(self._samples, 1)\n",
    "\n",
    "    @staticmethod\n",
    "    def _convert_samples_to_float32(samples):\n",
    "        \"\"\"Convert sample type to float32.\n",
    "        Audio sample type is usually integer or float-point.\n",
    "        Integers will be scaled to [-1, 1] in float32.\n",
    "        \"\"\"\n",
    "        float32_samples = samples.astype(\"float32\")\n",
    "        if samples.dtype in np.sctypes[\"int\"]:\n",
    "            bits = np.iinfo(samples.dtype).bits\n",
    "            float32_samples *= 1.0 / ((2 ** (bits - 1)) - 1)\n",
    "        elif samples.dtype in np.sctypes[\"float\"]:\n",
    "            pass\n",
    "        else:\n",
    "            raise TypeError(\"Unsupported sample type: %s.\" % samples.dtype)\n",
    "        return WAV_SCALE_FACTOR * float32_samples\n",
    "\n",
    "    @classmethod\n",
    "    def from_file(\n",
    "        cls,\n",
    "        filename,\n",
    "        target_sr=16000,\n",
    "        offset=0,\n",
    "        duration=0,\n",
    "        min_duration=0,\n",
    "        trim=False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Load a file supported by librosa and return as an AudioSegment.\n",
    "        :param filename: path of file to load\n",
    "        :param target_sr: the desired sample rate\n",
    "        :param int_values: if true, load samples as 32-bit integers\n",
    "        :param offset: offset in seconds when loading audio\n",
    "        :param duration: duration in seconds when loading audio\n",
    "        :return: numpy array of samples\n",
    "        \"\"\"\n",
    "        with sf.SoundFile(filename, \"r\") as f:\n",
    "            dtype_options = {\n",
    "                \"PCM_16\": \"int16\",\n",
    "                \"PCM_32\": \"int32\",\n",
    "                \"FLOAT\": \"float32\",\n",
    "            }\n",
    "            dtype_file = f.subtype\n",
    "            if dtype_file in dtype_options:\n",
    "                dtype = dtype_options[dtype_file]\n",
    "            else:\n",
    "                dtype = \"float32\"\n",
    "            sample_rate = f.samplerate\n",
    "            if offset > 0:\n",
    "                f.seek(int(offset * sample_rate))\n",
    "            if duration > 0:\n",
    "                samples = f.read(int(duration * sample_rate), dtype=dtype)\n",
    "            else:\n",
    "                samples = f.read(dtype=dtype)\n",
    "\n",
    "        num_zero_pad = int(target_sr * min_duration - samples.shape[0])\n",
    "        if num_zero_pad > 0:\n",
    "            samples = np.pad(samples, [0, num_zero_pad], mode=\"constant\")\n",
    "\n",
    "        samples = samples.transpose()\n",
    "        return cls(samples, sample_rate, target_sr=target_sr, trim=trim)\n",
    "\n",
    "    @property\n",
    "    def samples(self):\n",
    "        return self._samples.copy()\n",
    "\n",
    "    @property\n",
    "    def sample_rate(self):\n",
    "        return self._sample_rate\n",
    "\n",
    "\n",
    "# read audio chunk from a file\n",
    "def get_audio_chunk_from_soundfile(sf, chunk_size):\n",
    "\n",
    "    dtype_options = {\"PCM_16\": \"int16\", \"PCM_32\": \"int32\", \"FLOAT\": \"float32\"}\n",
    "    dtype_file = sf.subtype\n",
    "    if dtype_file in dtype_options:\n",
    "        dtype = dtype_options[dtype_file]\n",
    "    else:\n",
    "        dtype = \"float32\"\n",
    "    audio_signal = sf.read(chunk_size, dtype=dtype)\n",
    "    end = False\n",
    "    # pad to chunk size\n",
    "    if len(audio_signal) < chunk_size:\n",
    "        end = True\n",
    "        audio_signal = np.pad(\n",
    "            audio_signal, (0, chunk_size - len(audio_signal)), mode=\"constant\"\n",
    "        )\n",
    "    return audio_signal, end\n",
    "\n",
    "\n",
    "# generator that returns chunks of audio data from file\n",
    "def audio_generator_from_file(input_filename, target_sr, chunk_duration):\n",
    "\n",
    "    sf = soundfile.SoundFile(input_filename, \"rb\")\n",
    "    chunk_size = int(chunk_duration * sf.samplerate)\n",
    "    start = True\n",
    "    end = False\n",
    "\n",
    "    while not end:\n",
    "\n",
    "        audio_signal, end = get_audio_chunk_from_soundfile(sf, chunk_size)\n",
    "\n",
    "        audio_segment = AudioSegment(audio_signal, sf.samplerate, target_sr)\n",
    "\n",
    "        yield audio_segment.samples, target_sr, start, end\n",
    "        start = False\n",
    "\n",
    "    sf.close()\n",
    "\n",
    "\n",
    "def libri_generator(DATASET_ROOT):\n",
    "    for subdir in os.listdir(DATASET_ROOT):\n",
    "        SUBDIR = os.path.join(DATASET_ROOT, subdir)\n",
    "        if os.path.isdir(os.path.join(DATASET_ROOT, subdir)):\n",
    "            for subsubdir in os.listdir(SUBDIR):\n",
    "                SUBSUBDIR = os.path.join(SUBDIR, subsubdir)\n",
    "                # print(os.listdir(SUBSUBDIR))\n",
    "                transcription_file = os.path.join(\n",
    "                    DATASET_ROOT,\n",
    "                    SUBDIR,\n",
    "                    SUBSUBDIR,\n",
    "                    \"%s-%s.trans.txt\" % (subdir, subsubdir),\n",
    "                )\n",
    "                transcriptions = {}\n",
    "                # pdb.set_trace()\n",
    "                with open(transcription_file, \"r\") as f:\n",
    "                    for line in f:\n",
    "                        fields = line.split(\" \")\n",
    "                        transcriptions[fields[0]] = \" \".join(fields[1:])\n",
    "                for file_key, transcript in transcriptions.items():\n",
    "                    file_path = os.path.join(\n",
    "                        DATASET_ROOT, SUBDIR, SUBSUBDIR, file_key + \".wav\"\n",
    "                    )\n",
    "                    yield file_path, transcript.strip().lower()\n",
    "\n",
    "\n",
    "class UserData:\n",
    "    def __init__(self):\n",
    "        self._completed_requests = queue.Queue()\n",
    "\n",
    "\n",
    "# Define the callback function. Note the last two parameters should be\n",
    "# result and error. InferenceServerClient would povide the results of an\n",
    "# inference as grpcclient.InferResult in result. For successful\n",
    "# inference, error will be None, otherwise it will be an object of\n",
    "# tritonclientutils.InferenceServerException holding the error details\n",
    "def callback(user_data, result, error):\n",
    "    if error:\n",
    "        user_data._completed_requests.put(error)\n",
    "    else:\n",
    "        # print(result.as_numpy(\"TEXT\"))\n",
    "        user_data._completed_requests.put(result.as_numpy(\"TEXT\"))\n",
    "\n",
    "\n",
    "class Streamer():\n",
    "    def __init__(self, q):\n",
    "        self.q = q\n",
    "    def __iter__(self):\n",
    "        while not self.q._completed_requests.empty():\n",
    "            yield self.q._completed_requests.get_nowait().flatten()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4\"></a>\n",
    "## Offline Inference\n",
    "\n",
    "We first create an inference client that connects to the Kaldi TRTIS servier via a gPRC connection.\n",
    "\n",
    "The server expects chunks of audio each containing up to input.WAV_DATA.dims samples (default: 8160). Per default, this corresponds to 510ms of audio per chunk (i.e. 16000Hz sampling rate). The last chunk can send a partial chunk smaller than this maximum value.\n",
    "\n",
    "Next, we take chunks from a selected audio file (each 510ms in duration, containing 8160 samples) and stream them sequentially to the Kaldi server. The server processes each chunk as soon as it is received. The transcription result is stored in the callback function for each stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_data = UserData()\n",
    "sequence_id = 1\n",
    "count = 0\n",
    "result_list = []\n",
    "\n",
    "\n",
    "with grpcclient.InferenceServerClient(url=URL, verbose=VERBOSE) as triton_client:\n",
    "    try:\n",
    "        # Establish stream\n",
    "        triton_client.start_stream(callback=partial(callback, user_data))\n",
    "\n",
    "        for value_data in audio_generator_from_file(FILE_NAME, 16000, 0.51):\n",
    "            # Create the tensor for INPUT\n",
    "            wav_data = value_data[0]\n",
    "            dim = np.full(shape=[1, 1], fill_value=len(wav_data), dtype=np.int32)\n",
    "\n",
    "            wav_data = np.full(\n",
    "                shape=[BATCH_SIZE, 8160], fill_value=wav_data, dtype=np.float32\n",
    "            )\n",
    "\n",
    "            inputs = []\n",
    "            inputs.append(grpcclient.InferInput(\"WAV_DATA\", wav_data.shape, \"FP32\"))\n",
    "            inputs[-1].set_data_from_numpy(wav_data)\n",
    "\n",
    "            inputs.append(grpcclient.InferInput(\"WAV_DATA_DIM\", dim.shape, \"INT32\"))\n",
    "            inputs[-1].set_data_from_numpy(dim)\n",
    "\n",
    "            outputs = []\n",
    "            outputs.append(grpcclient.InferRequestedOutput(\"TEXT\"))\n",
    "\n",
    "            # Issue the asynchronous sequence inference.\n",
    "            result = triton_client.async_stream_infer(\n",
    "                model_name=MODEL_NAME,\n",
    "                model_version=MODEL_VERSION,\n",
    "                inputs=inputs,\n",
    "                outputs=outputs,\n",
    "                request_id=\"{}_{}\".format(sequence_id, count),\n",
    "                sequence_id=sequence_id,\n",
    "                sequence_start=value_data[2],\n",
    "                sequence_end=value_data[3],\n",
    "            )\n",
    "            count = count + 1\n",
    "\n",
    "    except InferenceServerException as error:\n",
    "        print(\"InferenceServerException: {}\".format(error))\n",
    "        \n",
    "        \n",
    "for item in Streamer(user_data):\n",
    "    print(item.decode('utf-8').lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batcmd = \"cat %s|grep %s\" % (TRANSRIPTION_FILE, FILE_ID)\n",
    "res = subprocess.check_output(batcmd, shell=True)\n",
    "transcript = \" \".join(res.decode(\"utf-8\").split(\" \")[1:]).lower()\n",
    "\n",
    "print(transcript)\n",
    "ipd.Audio(FILE_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Run Inference in Parallel\n",
    "___\n",
    "\n",
    "```bash\n",
    "mkdir /Kaldi/data/results\n",
    "kaldi-asr-parallel-client \\\n",
    "    -i 5 \\\n",
    "    -c 2000 \\\n",
    "    -o \\\n",
    "    -u 172.42.42.1:8001\n",
    "\n",
    "==================================================\n",
    "============= Triton Kaldi ASR Client ============\n",
    "==================================================\n",
    "\n",
    "Configuration:\n",
    "\n",
    "Number of iterations            : 5\n",
    "Number of parallel channels     : 2000\n",
    "Server URL                      : 172.42.42.1:8001\n",
    "Print text outputs              : No\n",
    "Print partial text outputs      : No\n",
    "Online - Realtime I/O           : Yes\n",
    "\n",
    "Loading eval dataset...done\n",
    "Loaded dataset with 2620 utterances, frequency 16000hz, total audio 19452.5 seconds\n",
    "Opening GRPC contexts...done\n",
    "Streaming utterances...\n",
    "..................................................................................................done\n",
    "Waiting for all results...done\n",
    "Latencies:      90%             95%             99%             Avg\n",
    "                0.109           0.115           0.13            0.0893\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "TensorFlow_UNet_Industrial_Colab_train_and_inference.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
